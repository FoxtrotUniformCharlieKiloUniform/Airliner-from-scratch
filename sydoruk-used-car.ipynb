{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01777949",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-09-11T17:49:53.532657Z",
     "iopub.status.busy": "2024-09-11T17:49:53.532282Z",
     "iopub.status.idle": "2024-09-11T17:49:58.966355Z",
     "shell.execute_reply": "2024-09-11T17:49:58.965209Z"
    },
    "papermill": {
     "duration": 5.442296,
     "end_time": "2024-09-11T17:49:58.968849",
     "exception": false,
     "start_time": "2024-09-11T17:49:53.526553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table names:  sample_submission.csv\n",
      "Table names:  train.csv\n",
      "Table names:  test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn import preprocessing \n",
    "import torch.nn as nn\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(\"Table names: \", filename)\n",
    "        fullPath = os.path.join(dirname, filename)\n",
    "        \n",
    "        \n",
    "        if filename == \"train.csv\":\n",
    "            trainDS = pd.read_csv(fullPath)\n",
    "        if filename == \"test.csv\":\n",
    "            testDS = pd.read_csv(fullPath)\n",
    "        \n",
    "        \n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c61d5bd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T17:49:58.978096Z",
     "iopub.status.busy": "2024-09-11T17:49:58.977717Z",
     "iopub.status.idle": "2024-09-11T17:49:58.983946Z",
     "shell.execute_reply": "2024-09-11T17:49:58.983043Z"
    },
    "papermill": {
     "duration": 0.013347,
     "end_time": "2024-09-11T17:49:58.986310",
     "exception": false,
     "start_time": "2024-09-11T17:49:58.972963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "## using hardware accelerators\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Using GPU: \", torch.cuda.get_device_name(0))\n",
    "elif 'XLA_USE_BF16' in os.environ:  # This is how TPUs are usually detected in Kaggle\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    device = xm.xla_device()\n",
    "    print(\"Using TPU\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baa8b97b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T17:49:58.995467Z",
     "iopub.status.busy": "2024-09-11T17:49:58.995062Z",
     "iopub.status.idle": "2024-09-11T17:49:59.011082Z",
     "shell.execute_reply": "2024-09-11T17:49:59.009940Z"
    },
    "papermill": {
     "duration": 0.023066,
     "end_time": "2024-09-11T17:49:59.013336",
     "exception": false,
     "start_time": "2024-09-11T17:49:58.990270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id brand          model  model_year  milage fuel_type  \\\n",
      "0   0  MINI  Cooper S Base        2007  213000  Gasoline   \n",
      "\n",
      "                                         engine transmission ext_col int_col  \\\n",
      "0  172.0HP 1.6L 4 Cylinder Engine Gasoline Fuel          A/T  Yellow    Gray   \n",
      "\n",
      "        accident clean_title  price  \n",
      "0  None reported         Yes   4200  \n"
     ]
    }
   ],
   "source": [
    "## The plan\n",
    "#Split up engine type into the following columns\n",
    "#HP, engine size (without the Liter), cylinder number (sometimes says X cylinder, sometimes VX or IX),fuel type\n",
    "\n",
    "#encode brand, model, fuel_type, ext_col, int_col, accident, clean_title\n",
    "\n",
    "#label is obviously price\n",
    "print(trainDS.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5da76eb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T17:49:59.022917Z",
     "iopub.status.busy": "2024-09-11T17:49:59.022289Z",
     "iopub.status.idle": "2024-09-11T17:49:59.031212Z",
     "shell.execute_reply": "2024-09-11T17:49:59.030217Z"
    },
    "papermill": {
     "duration": 0.015974,
     "end_time": "2024-09-11T17:49:59.033326",
     "exception": false,
     "start_time": "2024-09-11T17:49:59.017352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## Data preprocessing block (from above plan)\\n#print(trainDS.head(20))\\nprint(trainDS[\"engine\"].value_counts())\\nprint(len(trainDS) - trainDS[\\'engine\\'].str.contains(\"HP\").sum())\\nprint(trainDS[\\'engine\\'].str.contains(\"HP\").sum())\\n\\nunique_non_occurrences = trainDS[~trainDS[\\'engine\\'].str.contains(\\'HP\\')][\\'engine\\'].unique()\\nprint(unique_non_occurrences)'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Taking a look at what engines have HP in them or not\n",
    "'''## Data preprocessing block (from above plan)\n",
    "#print(trainDS.head(20))\n",
    "print(trainDS[\"engine\"].value_counts())\n",
    "print(len(trainDS) - trainDS['engine'].str.contains(\"HP\").sum())\n",
    "print(trainDS['engine'].str.contains(\"HP\").sum())\n",
    "\n",
    "unique_non_occurrences = trainDS[~trainDS['engine'].str.contains('HP')]['engine'].unique()\n",
    "print(unique_non_occurrences)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13fa9cea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T17:49:59.044382Z",
     "iopub.status.busy": "2024-09-11T17:49:59.043720Z",
     "iopub.status.idle": "2024-09-11T17:49:59.053760Z",
     "shell.execute_reply": "2024-09-11T17:49:59.052800Z"
    },
    "papermill": {
     "duration": 0.018465,
     "end_time": "2024-09-11T17:49:59.055888",
     "exception": false,
     "start_time": "2024-09-11T17:49:59.037423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    # Ensure the input is a DataFrame\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        print(\"Variable is not a dataframe.\")\n",
    "        raise SystemExit(\"Stopping execution due to incorrect input.\")\n",
    "\n",
    "    # Check if 'engine' column exists\n",
    "    columnSet = [\"engine\"]\n",
    "    if set(columnSet).issubset(df.columns):\n",
    "        print(\"'engine' column found, processing...\")\n",
    "        \n",
    "        # Split the 'engine' column\n",
    "        try:\n",
    "            columnAdditions = df[\"engine\"].str.split('HP', expand=True)\n",
    "            intermVar = columnAdditions[1].str.split(\"L\", expand=True)\n",
    "            columnAdditions = pd.concat([columnAdditions, intermVar], axis=1)\n",
    "\n",
    "            # Cleanup column names\n",
    "            columnAdditions.columns = [0, 1, 2, 3]  # Give temporary names\n",
    "            columnAdditions = columnAdditions.rename(columns={0: \"HP\", 2: \"engine_size\"})\n",
    "            columnAdditions = columnAdditions.drop([1, 3], axis=1)  # Drop unnecessary columns\n",
    "\n",
    "            # Convert to numeric values\n",
    "            columnAdditions[\"HP\"] = pd.to_numeric(columnAdditions[\"HP\"], errors='coerce')\n",
    "            columnAdditions[\"engine_size\"] = pd.to_numeric(columnAdditions[\"engine_size\"], errors='coerce')\n",
    "\n",
    "            # Merge back with the original DataFrame\n",
    "            newdf = pd.concat([df, columnAdditions], axis=1)\n",
    "            newdf = newdf.drop(\"engine\", axis=1)  # Drop the original 'engine' column\n",
    "\n",
    "            print(\"Processing complete. Returning new DataFrame.\")\n",
    "            return newdf\n",
    "\n",
    "        except KeyError as e:\n",
    "            print(f\"KeyError: {e} - One of the expected columns is missing.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "    else:\n",
    "        print(\"'engine' column is not in the DataFrame (it might have already been processed).\")\n",
    "        return df  # Return the original DataFrame if no processing was done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d51299c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T17:49:59.065852Z",
     "iopub.status.busy": "2024-09-11T17:49:59.065460Z",
     "iopub.status.idle": "2024-09-11T17:49:59.071581Z",
     "shell.execute_reply": "2024-09-11T17:49:59.070667Z"
    },
    "papermill": {
     "duration": 0.01357,
     "end_time": "2024-09-11T17:49:59.073651",
     "exception": false,
     "start_time": "2024-09-11T17:49:59.060081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encodeData(df, columnSet):\n",
    "    #make sure ds is dataframe\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        pass\n",
    "    else:\n",
    "        print(\"Variable is not dataframe\")\n",
    "        #raise SystemExit(\"Stop right there!\")\n",
    "       \n",
    "    #make sure column names are all valid \n",
    "    if set(columnSet).issubset(df.columns):\n",
    "        pass\n",
    "    else:\n",
    "        print(\"Col list invalid\")\n",
    "        #raise SystemExit(\"Stop right there!\")\n",
    "    \n",
    "    #encode all specified columns\n",
    "    for column in columnSet:\n",
    "        df[column] = label_encoder.fit_transform(df[column])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d10f5ecf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T17:49:59.084357Z",
     "iopub.status.busy": "2024-09-11T17:49:59.083441Z",
     "iopub.status.idle": "2024-09-11T17:50:00.888516Z",
     "shell.execute_reply": "2024-09-11T17:50:00.887569Z"
    },
    "papermill": {
     "duration": 1.813763,
     "end_time": "2024-09-11T17:50:00.891555",
     "exception": false,
     "start_time": "2024-09-11T17:49:59.077792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'engine' column found, processing...\n",
      "Processing complete. Returning new DataFrame.\n"
     ]
    }
   ],
   "source": [
    "#load and process data\n",
    "batch_size = 32\n",
    "\n",
    "class carDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "        #call engine preprocess method to preprocess the engines (split into HP and engine size)\n",
    "        self.df = preprocess(self.df)\n",
    "        self.df = encodeData(self.df, [\"model\", \"brand\", \"fuel_type\", \"transmission\", \"ext_col\", \"int_col\", \"accident\", \"clean_title\"])\n",
    "        \n",
    "        self.df = self.df.replace(np.nan,0)\n",
    "        \n",
    "        self.labels = self.df[[\"price\"]]   #y is just the price (this for )\n",
    "        self.features = self.df.drop([\"price\"], axis = 1)  #x is dataframe with price column dropped\n",
    "        \n",
    "        self.features = (self.features - self.features.mean())/self.features.std() \n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        # Convert the features and labels to PyTorch tensors\n",
    "        features_tensor = torch.tensor(self.features.iloc[idx], dtype=torch.float32).to(device)\n",
    "        labels_tensor = torch.tensor(self.labels.iloc[idx], dtype=torch.float32).to(device)\n",
    "        \n",
    "        return features_tensor, labels_tensor\n",
    " \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "carDataset = carDataset(trainDS)\n",
    "trainer = DataLoader(carDataset, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "885a31ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T17:50:00.906902Z",
     "iopub.status.busy": "2024-09-11T17:50:00.906085Z",
     "iopub.status.idle": "2024-09-11T17:50:00.912943Z",
     "shell.execute_reply": "2024-09-11T17:50:00.911881Z"
    },
    "papermill": {
     "duration": 0.016742,
     "end_time": "2024-09-11T17:50:00.915096",
     "exception": false,
     "start_time": "2024-09-11T17:50:00.898354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#initial random trees \\n\\nimport ydf\\nlearner = ydf.GradientBoostedTreesLearner(\\n    num_trees=15,\\n    label=\"price\",\\n    min_examples = 3,\\n    random_seed = 1,\\n)\\n\\ntrainDS = encodeData(preprocess(trainDS), [\"model\", \"brand\", \"fuel_type\", \"transmission\", \"ext_col\", \"int_col\", \"accident\", \"clean_title\"])\\n#print(trainDS.head)\\nmodel = learner.train(trainDS)\\ntestDS = encodeData(preprocess(testDS), [\"model\", \"brand\", \"fuel_type\", \"transmission\", \"ext_col\", \"int_col\", \"accident\", \"clean_title\"])\\npredict = model.predict(testDS)\\nmodel.describe()'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#initial random trees \n",
    "\n",
    "import ydf\n",
    "learner = ydf.GradientBoostedTreesLearner(\n",
    "    num_trees=15,\n",
    "    label=\"price\",\n",
    "    min_examples = 3,\n",
    "    random_seed = 1,\n",
    ")\n",
    "\n",
    "trainDS = encodeData(preprocess(trainDS), [\"model\", \"brand\", \"fuel_type\", \"transmission\", \"ext_col\", \"int_col\", \"accident\", \"clean_title\"])\n",
    "#print(trainDS.head)\n",
    "model = learner.train(trainDS)\n",
    "testDS = encodeData(preprocess(testDS), [\"model\", \"brand\", \"fuel_type\", \"transmission\", \"ext_col\", \"int_col\", \"accident\", \"clean_title\"])\n",
    "predict = model.predict(testDS)\n",
    "model.describe()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6aa88884",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T17:50:00.925535Z",
     "iopub.status.busy": "2024-09-11T17:50:00.924791Z",
     "iopub.status.idle": "2024-09-11T17:50:00.934228Z",
     "shell.execute_reply": "2024-09-11T17:50:00.933338Z"
    },
    "papermill": {
     "duration": 0.016995,
     "end_time": "2024-09-11T17:50:00.936435",
     "exception": false,
     "start_time": "2024-09-11T17:50:00.919440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66520493",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T17:50:00.947023Z",
     "iopub.status.busy": "2024-09-11T17:50:00.946122Z",
     "iopub.status.idle": "2024-09-11T17:50:02.269802Z",
     "shell.execute_reply": "2024-09-11T17:50:02.268808Z"
    },
    "papermill": {
     "duration": 1.331624,
     "end_time": "2024-09-11T17:50:02.272341",
     "exception": false,
     "start_time": "2024-09-11T17:50:00.940717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class usedCarModel(nn.Module):\n",
    "    def __init__(self, input_size, batch_size):\n",
    "        super(usedCarModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Define the layers\n",
    "        self.fc1 = nn.Linear(input_size, batch_size)  # First hidden layer\n",
    "        self.fc2 = nn.Linear(batch_size, 100)          # Second hidden layer\n",
    "        self.fc3 = nn.Linear(100, 1) # Output layer\n",
    "        \n",
    "        # Activation function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass\n",
    "        x = x.to(device)\n",
    "        #x = torch.Flatten(input)\n",
    "        x = self.relu(self.fc1(x))  # First layer + activation\n",
    "        x = self.relu(self.fc2(x))  # Second layer + activation\n",
    "        x = self.fc3(x)             # Output layer\n",
    "        #print(\"Average X output\", sum(x)/len(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "#model / hyperparameter definitions\n",
    "num_epochs = 18\n",
    "learning_rate = 0.01\n",
    "input_size = trainDS.shape[1]\n",
    "model = usedCarModel(input_size, batch_size).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "opt = optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9acf11a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T17:50:02.282931Z",
     "iopub.status.busy": "2024-09-11T17:50:02.282383Z",
     "iopub.status.idle": "2024-09-11T18:03:14.420336Z",
     "shell.execute_reply": "2024-09-11T18:03:14.419209Z"
    },
    "papermill": {
     "duration": 792.151075,
     "end_time": "2024-09-11T18:03:14.427886",
     "exception": false,
     "start_time": "2024-09-11T17:50:02.276811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17/1668757606.py:21: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  features_tensor = torch.tensor(self.features.iloc[idx], dtype=torch.float32).to(device)\n",
      "/tmp/ipykernel_17/1668757606.py:22: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  labels_tensor = torch.tensor(self.labels.iloc[idx], dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/18], Loss: 8137427825.2926, learning rate 0.01\n",
      "Epoch [2/18], Loss: 8137478846.6857, learning rate 0.01\n",
      "Epoch [3/18], Loss: 8137602058.5906, learning rate 0.01\n",
      "Epoch [4/18], Loss: 8137450637.5451, learning rate 0.01\n",
      "Epoch [5/18], Loss: 8137575809.7271, learning rate 0.01\n",
      "Epoch [6/18], Loss: 8137668742.1697, learning rate 0.01\n",
      "Epoch [7/18], Loss: 8137550388.6273, learning rate 0.01\n",
      "Epoch [8/18], Loss: 8137843853.1107, learning rate 0.01\n",
      "Epoch [9/18], Loss: 8148037753.9498, learning rate 0.01\n",
      "Epoch [10/18], Loss: 8137640875.3510, learning rate 0.01\n",
      "Epoch [11/18], Loss: 8137349224.1684, learning rate 0.01\n",
      "Epoch [12/18], Loss: 8137528237.9036, learning rate 0.01\n",
      "Epoch [13/18], Loss: 8137408061.9688, learning rate 0.01\n",
      "Epoch [14/18], Loss: 8138949988.6056, learning rate 0.01\n",
      "Epoch [15/18], Loss: 8144464065.1731, learning rate 0.01\n",
      "Epoch [16/18], Loss: 8137672116.2580, learning rate 0.01\n",
      "Epoch [17/18], Loss: 8137507881.6022, learning rate 0.01\n",
      "Epoch [18/18], Loss: 8137394683.0794, learning rate 0.01\n",
      "Mean epoch output:  [-0.13807349]\n"
     ]
    }
   ],
   "source": [
    "#Reduce LR Scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, 'min', factor = 0.7, patience = 2)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    total_output = []\n",
    "    \n",
    "    \n",
    "    for batch_idx, (inputs, labels) in enumerate(trainer):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs).to(device)\n",
    "       \n",
    "        total_output.append(outputs.cpu().detach().numpy())\n",
    "        \n",
    "        loss = criterion(outputs, labels.float())\n",
    "                         \n",
    "        # Backward pass and optimization\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        #opt.step()\n",
    "        scheduler.step(learning_rate)\n",
    "\n",
    "        # Accumulate loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate precision and recall\n",
    "\n",
    "    # Calculate average loss and accuracy for the epoch\n",
    "    epoch_loss = running_loss / len(trainer)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, learning rate {learning_rate}')\n",
    "    \n",
    "    mean_epoch_output = sum(outputs.cpu().detach().numpy()) / len(outputs.detach().numpy())\n",
    "    \n",
    "total_output = np.concatenate(total_output)\n",
    "\n",
    "print(\"Mean epoch output: \",mean_epoch_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fa99c4",
   "metadata": {
    "papermill": {
     "duration": 0.005187,
     "end_time": "2024-09-11T18:03:14.438667",
     "exception": false,
     "start_time": "2024-09-11T18:03:14.433480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9057646,
     "sourceId": 76728,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30761,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 804.696999,
   "end_time": "2024-09-11T18:03:15.667249",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-11T17:49:50.970250",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
